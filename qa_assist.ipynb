{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb117f5",
   "metadata": {
    "pinned": true
   },
   "source": [
    "# Updating Quality Docs from Emails\n",
    "\n",
    "I am one of only two employees of a medical device startup. So I am running every function: manufacturing, software, bench testing, clinical testing .... I progress in all these arenas primarily by emailing with contractors. There are two types of artifacts I need to maintain: \n",
    "\n",
    "* To keep track of everything, I keep a set of *detailed logs, one for each function*. I've been populating these by \n",
    "    1. cutting and pasting emails into a chatbot to get a cleaned-up text transcript of the conversation, then \n",
    "    2. pasting that transcript into the relevant log (a gdoc, with headers of each date). \n",
    "* I also keep a set of informal but foundational *quality documents: risk register, decision log, requirements list*. These are currently are google sheets. To maintain these, I've been feeding the logs to an openAI custom GPT which reads compares recent logs to the existing docs proposes new entries to the docs.\n",
    "I then review them myself and change their status to accepted. Each step has been manual and error prone, and the automated bits were vibe coded and obscure.\n",
    "\n",
    "What I *want* is for the _system_ to read my email every night, update the logs, and write proposed entries into the risk/requirements/decision registers for me to review -- from code I wrote and understand. Let's make it happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de33e9",
   "metadata": {},
   "source": [
    "## Google Cloud Deployment Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63718105",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This system automatically fetches emails from Gmail, cleans them using Claude AI, and updates Google Docs logs daily at 3:00 AM Central Time.\n",
    "\n",
    "#### Architecture\n",
    "- **Cloud Run Function**: Serverless function that processes emails and updates docs\n",
    "- **Cloud Scheduler**: Triggers the function daily at 3:00 AM CT\n",
    "- **Service Account**: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com` with domain-wide delegation\n",
    "- **Secret Manager**: Stores Anthropic API key securely\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 1: Project Setup\n",
    "\n",
    "##### 1.1 Create/Select Google Cloud Project\n",
    "1. Go to https://console.cloud.google.com\n",
    "2. Create new project or select existing: `qa-assistant-458920`\n",
    "3. Note your project ID for later steps\n",
    "\n",
    "##### 1.2 Enable Billing\n",
    "1. Navigate to **Billing** in Cloud Console\n",
    "2. Create or link a billing account\n",
    "3. Required even for free tier usage\n",
    "4. Set budget alert at $5.00 (warning only, doesn't cap charges)\n",
    "\n",
    "##### 1.3 Enable Required APIs\n",
    "Enable these APIs in **APIs & Services** â†’ **Library**:\n",
    "- Cloud Run API\n",
    "- Cloud Scheduler API\n",
    "- Secret Manager API\n",
    "- Gmail API\n",
    "- Google Docs API\n",
    "- IAM Service Account Credentials API\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 2: Service Account Configuration\n",
    "\n",
    "##### 2.1 Create Service Account\n",
    "1. Go to **IAM & Admin** â†’ **Service Accounts**\n",
    "2. Click **CREATE SERVICE ACCOUNT**\n",
    "3. Name: `docs-updater`\n",
    "4. Email will be: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "5. Click **CREATE AND CONTINUE**\n",
    "6. Skip role assignment for now\n",
    "7. Click **DONE**\n",
    "\n",
    "##### 2.2 Grant Service Account Token Creator Role\n",
    "**Why this is needed**: The service account needs to sign JWT tokens as itself for domain-wide delegation without downloading private keys. This approach keeps keys within Google's infrastructure and complies with security best practices.\n",
    "\n",
    "**Reference**: [Using domain-wide delegation without service account keys](https://jpassing.com/2022/01/15/using-domain-wide-delegation-on-google-cloud-without-service-account-keys/index.html)\n",
    "\n",
    "1. Go to **IAM & Admin** â†’ **Service Accounts**\n",
    "2. Click on `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "3. Click **PERMISSIONS** tab\n",
    "4. Click **Manage service account permissions**\n",
    "5. Add principal: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com` (yes, itself)\n",
    "6. Assign role: **Service Account Token Creator**\n",
    "7. Leave IAM conditions blank\n",
    "8. Click **SAVE**\n",
    "\n",
    "##### 2.3 Configure Domain-Wide Delegation\n",
    "1. Go to Google Workspace Admin Console: https://admin.google.com\n",
    "2. Navigate to **Security** â†’ **Access and data control** â†’ **API Controls**\n",
    "3. Click **MANAGE DOMAIN WIDE DELEGATION**\n",
    "4. Click **Add new**\n",
    "5. Client ID: Get from service account details (numeric ID, not email)\n",
    "6. OAuth Scopes (comma-separated):\n",
    "\n",
    "   https://www.googleapis.com/auth/gmail.readonly, \n",
    "   https://www.googleapis.com/auth/documents,\n",
    "   https://www.googleapis.com/auth/spreadsheets\n",
    "\n",
    "7. Click **AUTHORIZE**\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 3: Secret Manager Setup\n",
    "\n",
    "##### 3.1 Create Secret for Anthropic API Key\n",
    "1. Go to **Security** â†’ **Secret Manager**\n",
    "2. Click **CREATE SECRET**\n",
    "3. Name: `anthropic-api-key`\n",
    "4. Secret value: Your Anthropic API key\n",
    "5. Click **CREATE SECRET**\n",
    "\n",
    "##### 3.2 Grant Access to Service Account\n",
    "1. Click on the `anthropic-api-key` secret\n",
    "2. Click **PERMISSIONS** tab\n",
    "3. Click **GRANT ACCESS**\n",
    "4. Add principal: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "5. Assign role: **Secret Manager Secret Accessor**\n",
    "6. Click **SAVE**\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 4: Deploy Cloud Run Function\n",
    "\n",
    "##### 4.1 Prepare Code Files\n",
    "\n",
    "**main.py** - Contains your function code with these key components:\n",
    "- `get_google_service_keyless()` function using IAM signer (not downloaded keys)\n",
    "- `update_logs_daily()` as the entry point\n",
    "- Email fetching, cleaning, and document update logic\n",
    "\n",
    "**requirements.txt**:\n",
    "google-api-python-client\n",
    "google-auth\n",
    "claudette\n",
    "\n",
    "#### 4.2 Deploy Function\n",
    "1. Go to **Cloud Run** â†’ **Functions**\n",
    "2. Click **CREATE FUNCTION**\n",
    "3. Configuration:\n",
    "   - **Function name**: `update-logs-daily`\n",
    "   - **Region**: `us-central1` (Iowa)\n",
    "   - **Trigger type**: HTTPS\n",
    "   - **Authentication**: Require authentication\n",
    "   - **Runtime service account**: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "4. Click **NEXT**\n",
    "5. Runtime settings:\n",
    "   - **Runtime**: Python 3.12\n",
    "   - **Entry point**: `update_logs_daily`\n",
    "   - **Memory**: 512 MB (or adjust as needed)\n",
    "   - **Timeout**: 540 seconds\n",
    "6. Environment variables:\n",
    "   - Click **REFERENCE A SECRET**\n",
    "   - Secret: `anthropic-api-key`\n",
    "   - Exposed as: Environment variable\n",
    "   - Name: `ANTHROPIC_API_KEY`\n",
    "7. Copy your code into `main.py`\n",
    "8. Copy dependencies into `requirements.txt`\n",
    "9. Click **DEPLOY**\n",
    "\n",
    "##### 4.3 Grant Invoker Permission\n",
    "**Why this is needed**: Cloud Scheduler needs permission to trigger the function.\n",
    "\n",
    "1. From Cloud Run services list, check the box next to `update-logs-daily`\n",
    "2. Click **PERMISSIONS** at the top\n",
    "3. Click **ADD PRINCIPAL**\n",
    "4. New principal: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "5. Role: **Cloud Run Invoker**\n",
    "6. Click **SAVE**\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 5: Cloud Scheduler Setup\n",
    "\n",
    "##### 5.1 Create Scheduled Job\n",
    "1. Go to **Cloud Scheduler**\n",
    "2. Click **CREATE JOB**\n",
    "3. Configuration:\n",
    "   - **Name**: `daily-log-update`\n",
    "   - **Region**: `us-central1` (must match function region)\n",
    "   - **Frequency**: `0 3 * * *` (3:00 AM daily)\n",
    "   - **Timezone**: America/Chicago (Central Time)\n",
    "4. Configure the execution:\n",
    "   - **Target type**: HTTP\n",
    "   - **URL**: Your Cloud Run function URL (from function details page)\n",
    "   - **HTTP method**: POST\n",
    "   - **Auth header**: OIDC token\n",
    "   - **Service account**: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "5. Click **CREATE**\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 6: Testing\n",
    "\n",
    "##### 6.1 Manual Test\n",
    "1. Go to **Cloud Scheduler**\n",
    "2. Find `daily-log-update` job\n",
    "3. Click three dots â†’ **FORCE RUN**\n",
    "4. Check **Cloud Run** â†’ **Logs** for execution details\n",
    "\n",
    "##### 6.2 Verify Results\n",
    "1. Check your Google Docs logs for new entries\n",
    "2. Entries should be dated for yesterday (function processes previous day by default)\n",
    "\n",
    "---\n",
    "\n",
    "#### Troubleshooting\n",
    "\n",
    "##### HTTP 403 Error\n",
    "**Symptom**: Scheduler triggers but function returns 403\n",
    "**Cause**: Missing Cloud Run Invoker permission\n",
    "**Solution**: Follow Part 4.3 to grant invoker permission\n",
    "\n",
    "##### HTTP 500 Error: 'Credentials' object has no attribute 'with_subject'\n",
    "**Symptom**: Function executes but crashes with attribute error\n",
    "**Cause**: Using `google.auth.default()` with `.with_subject()` for delegation\n",
    "**Solution**: Use `google.auth.iam.Signer` approach (already in provided code)\n",
    "\n",
    "##### No Emails Processed\n",
    "**Symptom**: Function runs successfully but no log updates\n",
    "**Cause**: Function processes yesterday's emails by default\n",
    "**Solution**: Verify emails exist for the previous day, or manually specify date for testing\n",
    "\n",
    "---\n",
    "\n",
    "#### Cost Estimates\n",
    "- **Cloud Run**: Free tier includes 2 million requests/month\n",
    "- **Cloud Scheduler**: Free tier includes 3 jobs\n",
    "- **Secret Manager**: $0.06 per secret per month + $0.03 per 10,000 accesses\n",
    "- **Anthropic API**: Depends on usage (Claude Sonnet pricing)\n",
    "- **Expected monthly cost**: <$5 for typical usage\n",
    "\n",
    "---\n",
    "\n",
    "#### Security Notes\n",
    "1. **No downloaded keys**: Service account uses Google's IAM signJwt API instead of user-managed keys\n",
    "2. **Principle of least privilege**: Service account only has necessary permissions\n",
    "3. **Secret management**: API keys stored in Secret Manager, not in code\n",
    "4. **Authentication required**: All endpoints require IAM authentication\n",
    "5. **Internal only**: Function not exposed to public internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775158b",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "Ok I'm a little fuzzy. What are the steps I need to take to deploy this? It should be the same in many ways as with update-logs-daily. Maybe first I have to check my exported files are complete? Help me out :) \n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Good idea to start by checking your exported files! Since you've done this before with `update-logs-daily`, the process will be very similar.\n",
    "\n",
    "Let's go step by step. First, think about what functions your `update_qdocs_weekly` entry point needs to run. It calls:\n",
    "\n",
    "1. `update_all_docs` â†’ `get_logs`, `update`\n",
    "2. `get_logs` â†’ `get_gdocs_service`, `get_log_by_date`, etc.\n",
    "3. `update` â†’ `get_props`, `filter_props`, `qdoc.add_entries`\n",
    "4. The `QDoc` and `GSheet` classes\n",
    "5. The three qdoc instances (`rreg`, `dlog`, `dio`)\n",
    "\n",
    "Can you trace through and list out all the functions/classes that need to be in your `main.py`? \n",
    "\n",
    "Also, you'll need to decide: do you want this as a **separate** Cloud Run function, or add it to the same deployment as `update-logs-daily`? Both approaches work - separate is cleaner, same deployment shares code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1542e2",
   "metadata": {},
   "source": [
    "This had to be refactored slightly to move the creation of the qdocs (rreg, dio, dlog) to be called later rather than at module import:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65e30d0c",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from google.auth import default\n",
    "from googleapiclient.discovery import build\n",
    "from claudette import Chat\n",
    "\n",
    "from google.auth import iam\n",
    "from google.auth.transport import requests as google_requests\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from claudette import Client\n",
    "\n",
    "\n",
    "def get_google_service_keyless(service_name, version, scopes, user_email):\n",
    "    \"\"\"Get Google API service using service account with delegation via IAM signJwt API.\"\"\"\n",
    "    service_account_email = 'docs-updater@qa-assistant-458920.iam.gserviceaccount.com'\n",
    "    \n",
    "    # Get default credentials for signing\n",
    "    signing_creds, _ = default()\n",
    "    \n",
    "    # Create signer that uses IAM API\n",
    "    signer = iam.Signer(\n",
    "        request=google_requests.Request(),\n",
    "        credentials=signing_creds,\n",
    "        service_account_email=service_account_email\n",
    "    )\n",
    "    \n",
    "    # Create delegated credentials\n",
    "    delegated_creds = service_account.Credentials(\n",
    "        signer=signer,\n",
    "        service_account_email=service_account_email,\n",
    "        token_uri='https://oauth2.googleapis.com/token',\n",
    "        scopes=scopes,\n",
    "        subject=user_email\n",
    "    )\n",
    "    \n",
    "    return build(service_name, version, credentials=delegated_creds)\n",
    "\n",
    "\n",
    "\n",
    "def get_gdocs_service_keyless():\n",
    "    return get_google_service_keyless(\n",
    "        'docs', \n",
    "        'v1', \n",
    "        ['https://www.googleapis.com/auth/documents'],\n",
    "        'dan@onehealthbiosensing.com'\n",
    "    )\n",
    "    \n",
    "def get_gsheets_service_keyless():\n",
    "    return get_google_service_keyless(\n",
    "        'sheets', \n",
    "        'v4', \n",
    "        ['https://www.googleapis.com/auth/spreadsheets'],\n",
    "        'dan@onehealthbiosensing.com'\n",
    "    )\n",
    "\n",
    "\n",
    "# Log configuration\n",
    "LOGS = {\n",
    "    \"assembly\": {\n",
    "        \"log_doc_id\": \"1Jb1LEU_VVorhIkslgkGOUgRADny23Z1Q310GJSo6ZWg\",\n",
    "        \"label_id\": \"Label_3360747663054290481\"\n",
    "    },\n",
    "    \"wcp\": {\n",
    "        \"log_doc_id\": \"1S4te4jvQGokB4cSbbInO9HuQUCEY6MHRontdIplXOBo\",\n",
    "        \"label_id\": \"Label_1806033690653380900\"\n",
    "    },\n",
    "    \"bench\": {\n",
    "        \"log_doc_id\": \"1ezsnTsRJQE8NkVId5sOqXowKopygoov9xzIl35oKczk\",\n",
    "        \"label_id\": \"Label_2306226108971101222\"\n",
    "    },\n",
    "    \"singulation\": {\n",
    "        \"log_doc_id\": \"1C6pDV0YWWwQra3sxPSRatdNV8DjPPdeBOvzxylLIUok\",\n",
    "        \"label_id\": \"Label_6946365540848423185\"\n",
    "    },\n",
    "    \"sterilization\": {\n",
    "        \"log_doc_id\": \"1HUfc25DiOzTIVqnFQ0FEqNqEaxs7eTosMxVKJCov9KQ\",\n",
    "        \"label_id\": \"Label_2566682383777891792\"\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "            \"log_doc_id\": \"1nxCES9tzy2ZC0aFN-B08LO5lSs4YYVsgplED0v6GXxU\",\n",
    "            \"label_id\": \"Label_4176937437539633686\"\n",
    "    },\n",
    "    \"firmware\": {\n",
    "        \"log_doc_id\": \"1KF8LlS0UbJCdVbcqp_7BEwScJQc4EHwfI6fxNCU-SKE\",\n",
    "        \"label_id\": None\n",
    "    },\n",
    "    \"quality\": {\n",
    "        \"log_doc_id\": \"1uXwbWHT3PjMFg6CJCIPs8ttQYTEdE4SYqm8wmH_AIvA\",\n",
    "        \"label_id\": None\n",
    "    },\n",
    "    \"clinicals\": {\n",
    "        \"log_doc_id\": \"1EXJ4kd531ewCqZpD-aPqKdlMfVmRmQ9LIRsppUhSLuk\",\n",
    "        \"label_id\": None\n",
    "    },\n",
    "    \"submission\": {\n",
    "        \"log_doc_id\": \"1Bbr8U9iDfYPrnoFKNeMgeMxYTN6uyeTTan5cvX_WmDI\",\n",
    "        \"label_id\": None\n",
    "    }\n",
    "}\n",
    "def log_to_dict_by_date(parts_list):\n",
    "    log_dict = {}\n",
    "    key = None\n",
    "    year = None\n",
    "    for part in parts_list:\n",
    "        content = get_content(part).strip()\n",
    "        if is_heading_2(part):\n",
    "            year = int(content)\n",
    "        if is_heading_3(part) and content:\n",
    "            key = log_header_to_date(content, year)\n",
    "            log_dict[key] = ''\n",
    "        else: \n",
    "            if key is not None: \n",
    "                log_dict[key] += content\n",
    "    return log_dict\n",
    "    \n",
    "def get_full_gdoc(service,doc_id):\n",
    "    return service.documents().get(documentId=doc_id).execute()\n",
    "\n",
    "def get_gdoc_body_contents(gdoc):\n",
    "    '''\n",
    "    gdoc is the full documents.get() result\n",
    "    returns a list of parts (paragraph or heading or break)\n",
    "    '''\n",
    "    return gdoc['body']['content']\n",
    "\n",
    "def get_paragraph_style(part):\n",
    "    return (part.get('paragraph', {})\n",
    "                .get('paragraphStyle', {})\n",
    "                .get('namedStyleType'))\n",
    "\n",
    "def is_heading_2(part):\n",
    "    return get_paragraph_style(part)=='HEADING_2'\n",
    "\n",
    "def is_heading_3(part):\n",
    "    return get_paragraph_style(part)=='HEADING_3'\n",
    "\n",
    "def get_content(part):\n",
    "    els = part.get('paragraph',{}).get('elements','')\n",
    "    return ''.join([e.get('textRun',{}).get('content','') for e in els])\n",
    "    \n",
    "def log_header_to_date(content, year=None):\n",
    "    if year is None:\n",
    "        year = datetime.now().year\n",
    "    \n",
    "    formats = [\n",
    "        (content,'%Y/%m/%d'),\n",
    "        (content,'%Y-%m-%d'),\n",
    "        (content,'%m/%d/%y'),\n",
    "        (f'{year}/{content}','%Y/%m/%d'),\n",
    "        (f'{content} {year}', '%d %b %Y'),\n",
    "        (f'{content} {year}', '%d %B %Y'),\n",
    "        (f'{content} {year}', '%b %d %Y'),\n",
    "        (f'{content} {year}', '%B %d %Y')\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(fmt[0], fmt[1]).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    raise ValueError(f\"Could not parse date: {content}\")\n",
    "\n",
    "def get_log_entries_in_range(log_dict,start_date,end_date):\n",
    "    return {str(k): v for k,v in log_dict.items() #str(date) for json\n",
    "            if start_date <= k <= end_date}\n",
    "            \n",
    "def get_log_by_date(service,log_key, start_date, end_date):\n",
    "    log_id = LOGS[log_key]['log_doc_id']\n",
    "    full_doc = get_full_gdoc(service, log_id)\n",
    "    parts_list = get_gdoc_body_contents(full_doc)\n",
    "    log_dict = log_to_dict_by_date(parts_list)\n",
    "    return get_log_entries_in_range(log_dict, start_date, end_date)\n",
    "\n",
    "def get_logs(start_date,end_date):\n",
    "    gdocs=get_gdocs_service_keyless()\n",
    "    return {k: get_log_by_date(gdocs,k,start_date,end_date)\n",
    "            for k in list(LOGS)}\n",
    "\n",
    "class GSheet:\n",
    "    \"A google sheet\"\n",
    "    def __init__(self,sheet_id:str,tab_range:str):\n",
    "        self.sheet_id=sheet_id # gsheet document id\n",
    "        self.tab_range=tab_range #gsheet tab & range\n",
    "\n",
    "    def get_rows(self):\n",
    "        gsheets=get_gsheets_service_keyless()\n",
    "        result = gsheets.spreadsheets().values().get(\n",
    "            spreadsheetId=self.sheet_id,\n",
    "            range=self.tab_range\n",
    "        ).execute()\n",
    "        return result.get('values',[])\n",
    "\n",
    "    def append(self,rows):\n",
    "        gsheets=get_gsheets_service_keyless()\n",
    "        gsheets.spreadsheets().values().append(\n",
    "            spreadsheetId=self.sheet_id,\n",
    "            range=self.tab_range,  \n",
    "            valueInputOption='RAW',  # or 'USER_ENTERED'\n",
    "            body={'values': rows}\n",
    "        ).execute()\n",
    "\n",
    "class QDoc(GSheet):\n",
    "    \"Quality Document (Risk Register, Decision Log, Design I/O Matrix)\"\n",
    "    def __init__(self, sheet_id, tab_range, entry_f):\n",
    "        super().__init__(sheet_id, tab_range)\n",
    "        self.entry_f = entry_f #doc entry defn function\n",
    "        self.rows = self.get_rows()\n",
    "        self._headers = self.rows[0] if self.rows else []\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_by_date_if_present(prop_dicts):\n",
    "        if not prop_dicts or 'Date' not in prop_dicts[0]: return prop_dicts\n",
    "        return sorted(prop_dicts, key=lambda d: d.get('Date', ''))\n",
    "\n",
    "    def _mk_row(self,prop):\n",
    "        return [prop.get(h,'') for h in self._headers]\n",
    "\n",
    "    def add_entries(self,props):\n",
    "        props=self._sort_by_date_if_present(props)\n",
    "        rows=[self._mk_row(prop) for prop in props]\n",
    "        self.append(rows)\n",
    "\n",
    "def risk_f(hazard:str, harms:str, causes:str)->dict:\n",
    "    \"Candidate Risk Register entry\"\n",
    "    return {'Hazard': hazard,\n",
    "        'Potential Harms': harms,\n",
    "        'Possible Causes': causes,\n",
    "        'Reviewed': 'FALSE'}\n",
    "\n",
    "\n",
    "\n",
    "def decision_f(component:str,decision:str,rationale:str,date:str)->dict:\n",
    "    \"Candidate Decision Log entry\"\n",
    "    return {'Component': component,\n",
    "        'Decision': decision,\n",
    "        'Rationale': rationale,\n",
    "        'Date': date,\n",
    "        'Reviewed': 'FALSE'}\n",
    "\n",
    "\n",
    "def design_f(design_input:str,source_rationale:str,design_output:str,verif_valid_method:str)->dict:\n",
    "    \"Candidate Design Input/Output Matrix entry\"\n",
    "    return {'Design Input': design_input,\n",
    "        'Source / Rationale': source_rationale,\n",
    "        'Design Output': design_output,\n",
    "        'Verification/Validation Method': verif_valid_method,\n",
    "        'Reviewed': 'FALSE'}\n",
    "\n",
    "\n",
    "\n",
    "def entries_prompt(logs):\n",
    "    p = f'''Review this set of engineering logs from a continuous glucose monitor development project\n",
    "            and identify candidate entries to the quality document represented in the tool. Focus on items \n",
    "            of significant, long-term importance; ignore short-term operational or execution issues. Keep \n",
    "            each field terse (10-20 words max).\n",
    "\n",
    "            Logs:\n",
    "            {logs} \n",
    "        '''\n",
    "    return p\n",
    "\n",
    "def get_props(logs, qdoc):\n",
    "    '''propose entries for qdoc before seeing existing entries''' \n",
    "    c = Client('claude-sonnet-4-5')\n",
    "    return c.structured(entries_prompt(logs),[qdoc.entry_f])\n",
    "\n",
    "def filter_props(props, qdoc):\n",
    "    p = f\"\"\"\n",
    "    Determine which proposed new entries are NOT already documented the given existing document. \n",
    "    Return ONLY those proposals that represent genuinely NEW concepts not already captured in existing entries. \n",
    "    - Ignore wording differences - focus on whether the core concept is already documented.\n",
    "    - If a proposal is semantically similar to an existing entry, exclude it.\n",
    "    - If NONE of the proposals are new, make NO tool calls.\n",
    "\n",
    "    Proposals:\n",
    "    {props}\n",
    "\n",
    "    Existing entries:\n",
    "    {qdoc.rows}\n",
    "    \"\"\"\n",
    "    c = Client('claude-sonnet-4-5')\n",
    "    return c.structured(p,qdoc.entry_f)\n",
    "\n",
    "\n",
    "def update(qdoc,logs):\n",
    "    props = get_props(logs,qdoc)\n",
    "    news = filter_props(props,qdoc)\n",
    "    if news: qdoc.add_entries(news)\n",
    "\n",
    "def get_qdocs():\n",
    "    rreg = QDoc(\n",
    "        sheet_id='1j752K9N89Qv9WLkjje3OWkrzDbSMjLwYuxdoTfArcpQ',\n",
    "        tab_range='Risk Register!A:L',\n",
    "        entry_f=risk_f\n",
    "    )\n",
    "    dlog = QDoc(\n",
    "        sheet_id='14oGD5hwGP_8OLfq_tqQVdEjicRzJ4cyQWPkXz92avv4',\n",
    "        tab_range='Decision Log!A:G',\n",
    "        entry_f=decision_f\n",
    "    )   \n",
    "    dio = QDoc(\n",
    "        sheet_id='1xrV_39iaVoTpAXmbxbKfFBBnbdHN5yeyKJMDZ59MdqM',\n",
    "        tab_range='Design IO Matrix!A:G',\n",
    "        entry_f=design_f\n",
    "    )\n",
    "    return [rreg,dlog,dio]\n",
    "\n",
    "def update_all_docs(start_date,end_date):\n",
    "    logs=get_logs(start_date,end_date)\n",
    "    if not any(logs.values()): return\n",
    "    qdocs = get_qdocs()\n",
    "    for qdoc in qdocs: \n",
    "        update(qdoc,logs)\n",
    "\n",
    "\n",
    "def update_qdocs_weekly(request):\n",
    "    \"To be run Friday mornings at 4am for the previous week (Fr-Th)\"\n",
    "    now = datetime.now(ZoneInfo('US/Central'))\n",
    "    end_date = (now - timedelta(hours=6)).date() #from 4h ago, with a margin\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    update_all_docs(start_date,end_date)\n",
    "    return f\"Quality docs updated through {end_date.strftime('%d/%m/%Y')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23416a71",
   "metadata": {
    "collapsed": true,
    "solveit_ai": true
   },
   "source": [
    "Success! Please review this dialogue and make a checklist of things to verify when depolying update-qdocs-weekly to google cloud run function / scheduler\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's your deployment checklist for `update-qdocs-weekly`:\n",
    "\n",
    "**Code Preparation:**\n",
    "- [ ] Replace `get_gdocs_service()` with `get_gdocs_service_keyless()`\n",
    "- [ ] Replace `get_gsheets_service()` with `get_gsheets_service_keyless()`\n",
    "- [ ] Move `QDoc` instance creation into `get_qdocs()` function (not at module level)\n",
    "- [ ] Prepare `requirements.txt` with: `google-api-python-client`, `google-auth`, `claudette`\n",
    "\n",
    "**Cloud Run Function Setup:**\n",
    "- [ ] Function name: `update-qdocs-weekly`\n",
    "- [ ] Region: `us-central1`\n",
    "- [ ] Runtime: Python 3.12\n",
    "- [ ] Entry point: `update_qdocs_weekly`\n",
    "- [ ] **Runtime service account**: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com` (NOT default!)\n",
    "- [ ] Authentication: Require authentication\n",
    "- [ ] Add secret reference: `anthropic-api-key` as environment variable `ANTHROPIC_API_KEY`\n",
    "\n",
    "**Permissions:**\n",
    "- [ ] Grant **Cloud Run Invoker** role to `docs-updater` service account on the function\n",
    "- [ ] Verify **Service Account Token Creator** role exists on `docs-updater` (should already be there from first function)\n",
    "- [ ] Verify **IAM Service Account Credentials API** is enabled\n",
    "\n",
    "**Cloud Scheduler:**\n",
    "- [ ] Job name: `weekly-qdocs-update`\n",
    "- [ ] Frequency: `0 10 * * 5` (Fridays 4am CST / 5am CDT)\n",
    "- [ ] Timezone: UTC\n",
    "- [ ] Target: HTTP POST to function URL\n",
    "- [ ] Auth: OIDC token with `docs-updater` service account\n",
    "\n",
    "**Test:**\n",
    "- [ ] Force run from scheduler\n",
    "- [ ] Check logs for errors\n",
    "- [ ] Verify entries appear in quality docs\n",
    "\n",
    "The key gotcha: runtime service account must be set explicitly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01b7e95",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Please read through the above and create a concise but complete checklist of the google-cloud-api-side steps that must be taken to get these up and running as google cloud functions. Both are actually running now, so the purpose of this checklist is as documentation/reference later if I need to change anything and need to remember how it was set up. \n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Here's a concise reference checklist for Google Cloud setup:\n",
    "\n",
    "**One-Time Project Setup:**\n",
    "- [ ] Enable APIs: Cloud Run, Cloud Scheduler, Secret Manager, Gmail, Google Docs, Google Sheets, IAM Service Account Credentials\n",
    "- [ ] Enable billing and set budget alert\n",
    "\n",
    "**Service Account Configuration:**\n",
    "- [ ] Create service account: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "- [ ] Grant **Service Account Token Creator** role to itself (for keyless JWT signing)\n",
    "- [ ] Configure domain-wide delegation in Google Workspace Admin Console with scopes: `gmail.readonly`, `documents`, `spreadsheets`\n",
    "\n",
    "**Secret Management:**\n",
    "- [ ] Create secret `anthropic-api-key` in Secret Manager\n",
    "- [ ] Grant `docs-updater` service account **Secret Manager Secret Accessor** role\n",
    "\n",
    "**Per-Function Deployment:**\n",
    "- [ ] Deploy function with runtime service account set to `docs-updater`\n",
    "- [ ] Reference `anthropic-api-key` as environment variable\n",
    "- [ ] Grant `docs-updater` **Cloud Run Invoker** role on the deployed function\n",
    "\n",
    "**Scheduler Setup:**\n",
    "- [ ] Create job with OIDC auth using `docs-updater` service account\n",
    "- [ ] Set schedule and timezone appropriately\n",
    "\n",
    "Does this capture what you need, or would you like me to add/modify anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37edd4c2",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "## Organize!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f60072",
   "metadata": {
    "collapsed": true,
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "Now this is up and running on google cloud, I want to set google cloud aside and think about refactoring, organizing and documenting this code here in solveit. I have two objectives for \"organized code\":\n",
    "1. Finish refactoring this into small set of modules, ideally each no longer than 50-60 lines, with clear separation of concerns and small functions that readable and well named requiring little documentation to understand what they do.\n",
    "2. Then create a dialogue behind each module \"nbdev style\". Some dialogue messages export to/sync with the corresponding module. What this adds (I think) is test calls and markdown notes for elements of the code which show tests for some functions and also make it clear for anyone who wants to modify the code for other uses what's going on. \n",
    "\n",
    "The current state of this dialogue is a mix of attempts at both and does neither successfully. It's half exported-to-modules and half scattered-fragments-through-this-dialogue. For the two modules already written, should I now delete those from this dialogue to simplify? Etc. \n",
    "\n",
    "I want to make it nice. Please suggest a plan.\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Great goal! Let me suggest a plan.\n",
    "\n",
    "**Step 1: Identify your logical modules**\n",
    "\n",
    "Based on what I see, here are natural boundaries:\n",
    "\n",
    "| Module | Responsibility | ~Lines |\n",
    "|--------|----------------|--------|\n",
    "| `google_auth.py` | Auth helpers (keyless + local) | ~50 |\n",
    "| `logs.py` | Read/parse log gdocs | ~50 |\n",
    "| `email_digest.py` | Fetch/clean emails â†’ logs | ~60 |\n",
    "| `qdocs.py` | GSheet, QDoc classes, entry functions | ~60 |\n",
    "| `proposals.py` | LLM calls: get_props, filter_props | ~30 |\n",
    "| `main.py` | Entry points for Cloud Run | ~40 |\n",
    "\n",
    "Does that breakdown feel right to you? Would you group things differently?\n",
    "\n",
    "**Step 2: Clean up this dialogue**\n",
    "\n",
    "Yes, I'd suggest:\n",
    "- Delete the code cells that were already exported to `google_auth.py` and `logs.py`\n",
    "- Keep only the \"live\" working code here temporarily while you reorganize\n",
    "\n",
    "**Step 3: Create one dialogue per module**\n",
    "\n",
    "Each dialogue would have:\n",
    "- A note explaining the module's purpose\n",
    "- Code cells that export to the `.py` file\n",
    "- Test cells (not exported) showing usage\n",
    "- Markdown notes explaining non-obvious bits\n",
    "\n",
    "**Question before we proceed:**\n",
    "\n",
    "Do you want to start by agreeing on the module structure, or do you want to first audit what's currently exported vs. scattered? I can help you trace through if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da9a46",
   "metadata": {
    "time_run": "2025-12-02T06:01:52.312574+00:00"
   },
   "outputs": [],
   "source": [
    "from fastcore.foundation import Path\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af69f15",
   "metadata": {
    "hide_input": true,
    "time_run": "2025-12-02T06:01:52.339586+00:00"
   },
   "outputs": [],
   "source": [
    "utils_code='''\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "DEF_TZ = 'US/Central'\n",
    "DEF_FMT = '%Y/%m/%d' \n",
    "\n",
    "def ytd(tz=DEF_TZ, fmt=DEF_FMT):\n",
    "    now = datetime.now(ZoneInfo(tz))\n",
    "    return now-timedelta(days=1)\n",
    "\n",
    "def str_to_date(content, year=None):\n",
    "    if year is None:\n",
    "        year = datetime.now().year\n",
    "\n",
    "    formats = [\n",
    "        (content,'%Y/%m/%d'),\n",
    "        (content,'%Y-%m-%d'),\n",
    "        (content,'%m/%d/%y'),\n",
    "        (f'{year}/{content}','%Y/%m/%d'),\n",
    "        (f'{content} {year}', '%d %b %Y'),\n",
    "        (f'{content} {year}', '%d %B %Y'),\n",
    "        (f'{content} {year}', '%b %d %Y'),\n",
    "        (f'{content} {year}', '%B %d %Y')\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(fmt[0], fmt[1]).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Could not parse date: {content}\")\n",
    "\n",
    "def parse_date(date,fmt=DEF_FMT,timezone=DEF_TZ):\n",
    "    tz = ZoneInfo(timezone)\n",
    "    return datetime.strptime(date, fmt).replace(tzinfo=tz)\n",
    "\n",
    "def add_days(date,delta:int):\n",
    "    return date+timedelta(days=delta)\n",
    "    \n",
    "def date_to_ms(date):\n",
    "    return int(date.timestamp() * 1000)\n",
    "\n",
    "def start_stop_ms(date,days,fmt=DEF_FMT,timezone=DEF_TZ):\n",
    "    start_date=parse_date(date,fmt,timezone)\n",
    "    end_date=add_days(start_date,days)\n",
    "    return date_to_ms(start_date), date_to_ms(end_date)\n",
    "'''\n",
    "Path('date_utils.py').write_text(utils_code)\n",
    "import date_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04b95c",
   "metadata": {
    "pinned": true,
    "time_run": "2025-12-02T06:01:52.365023+00:00"
   },
   "outputs": [],
   "source": [
    "from date_utils import ytd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937568c6",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "What's happening here?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "You're hitting a Python module caching issue! When you first imported `date_utils`, Python cached the empty (or previous) version. Writing new code to the file doesn't automatically reload it.\n",
    "\n",
    "Try this:\n",
    "\n",
    "```python\n",
    "import importlib\n",
    "importlib.reload(date_utils)\n",
    "from date_utils import ytd\n",
    "```\n",
    "\n",
    "Or restart your Python interpreter (the \"Clear\" button) and run the cells again from top to bottom.\n",
    "\n",
    "Does that fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1a7a3",
   "metadata": {
    "hide_input": true,
    "pinned": true,
    "time_run": "2025-12-02T06:02:04.248045+00:00"
   },
   "outputs": [],
   "source": [
    "auth_code=r\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google_auth_oauthlib.flow import Flow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_google_service(service_name, version, scopes, token_file):\n",
    "    '''Authenticate and return a Google API service.'''\n",
    "    creds = None\n",
    "    \n",
    "    if os.path.exists(token_file):\n",
    "        with open(token_file, 'rb') as token:\n",
    "            creds = pickle.load(token)\n",
    "    \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            client_config = {\n",
    "                \"web\": {\n",
    "                    \"client_id\": os.environ.get('QA_ASST_CLIENT_ID'),\n",
    "                    \"client_secret\": os.environ.get('QA_ASST_CLIENT_SECRET'),\n",
    "                    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "                    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "                    \"redirect_uris\": [\"http://localhost:8080\"]\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            flow = Flow.from_client_config(\n",
    "                client_config, \n",
    "                scopes=scopes\n",
    "            )\n",
    "            flow.redirect_uri = 'http://localhost:8080'\n",
    "            \n",
    "            auth_url, state = flow.authorization_url(\n",
    "                access_type='offline',\n",
    "                prompt='consent'\n",
    "            )\n",
    "            \n",
    "            print(\"Visit this URL to authorize:\")\n",
    "            print(auth_url)\n",
    "            print(\"\\nAfter authorizing, paste the full redirect URL here:\")\n",
    "            redirect_response = input()\n",
    "            \n",
    "            flow.fetch_token(authorization_response=redirect_response)\n",
    "            creds = flow.credentials\n",
    "        \n",
    "        with open(token_file, 'wb') as token:\n",
    "            pickle.dump(creds, token)\n",
    "    \n",
    "    return build(service_name, version, credentials=creds)\n",
    "\n",
    "def get_gmail_service():\n",
    "    '''Authenticate and return Gmail API service.'''\n",
    "    return get_google_service(\n",
    "        'gmail', \n",
    "        'v1', \n",
    "        ['https://www.googleapis.com/auth/gmail.readonly'],\n",
    "        'gmail_token.pickle'\n",
    "    )\n",
    "    \n",
    "def get_gdocs_service():\n",
    "    '''Authenticate and return GDocs API service.'''\n",
    "    return get_google_service(\n",
    "        'docs', \n",
    "        'v1', \n",
    "        ['https://www.googleapis.com/auth/documents'],\n",
    "        'docs_token.pickle'\n",
    "    )\n",
    "\n",
    "def get_gsheets_service():    \n",
    "    '''Authenticate and return Google Sheets API service.'''\n",
    "    return get_google_service(\n",
    "        'sheets',\n",
    "        'v4',\n",
    "        ['https://www.googleapis.com/auth/spreadsheets'],\n",
    "        'sheets_token.pickle'\n",
    "    )\n",
    "\"\"\"\n",
    "Path('google_auth.py').write_text(auth_code)\n",
    "import google_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b0790",
   "metadata": {
    "time_run": "2025-12-02T06:02:06.724697+00:00"
   },
   "outputs": [],
   "source": [
    "from google_auth import get_gsheets_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115771bf",
   "metadata": {
    "time_run": "2025-12-02T06:03:50.462492+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<googleapiclient.discovery.Resource at 0x70f685a900b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gsheets_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57761192",
   "metadata": {
    "hide_input": true,
    "time_run": "2025-12-02T06:03:50.512334+00:00"
   },
   "outputs": [],
   "source": [
    "emails_code=r'''\n",
    "from date_utils import start_stop_ms\n",
    "from google_auth import get_gmail_service\n",
    "\n",
    "# Email parsing\n",
    "def find_text_plain(payload):\n",
    "    \"\"\"Recursively search for text/plain part in email payload.\"\"\"\n",
    "    if payload.get('mimeType') == 'text/plain':\n",
    "        return payload.get('body', {}).get('data')\n",
    "    \n",
    "    if 'parts' in payload:\n",
    "        for part in payload['parts']:\n",
    "            result = find_text_plain(part)\n",
    "            if result:\n",
    "                return result\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_message_text(service, message_id):\n",
    "    \"\"\"Get plain text body, timestamp, threadId, and sender from a Gmail message.\"\"\"\n",
    "    import base64\n",
    "    msg = service.users().messages().get(userId='me', id=message_id).execute()\n",
    "    sender = None\n",
    "    for header in msg['payload']['headers']:\n",
    "        if header['name'] == 'From': \n",
    "            sender = header['value']\n",
    "    text_data = find_text_plain(msg['payload'])\n",
    "    if not text_data: return None, None, None, None\n",
    "\n",
    "    decoded = base64.urlsafe_b64decode(text_data).decode('utf-8')\n",
    "    timestamp = msg['internalDate']\n",
    "    threadId = msg['threadId']\n",
    "    return decoded, timestamp, threadId, sender\n",
    "    \n",
    "def extract_new_content(msg_text):\n",
    "    \"\"\"Extract new content from email, removing quoted replies.\"\"\"\n",
    "    markers = [\"\\r\\n\\r\\nOn \", \"\\r\\nFrom: \"]\n",
    "    positions = [msg_text.find(mark) for mark in markers]\n",
    "    valid_positions = [p for p in positions if p != -1]\n",
    "    if valid_positions:\n",
    "        split_point = min(valid_positions)\n",
    "        return msg_text[:split_point]\n",
    "    return msg_text\n",
    "\n",
    "def get_thread_ids(service,label_id):\n",
    "    thread_list_json = service.users().threads().list(\n",
    "        userId='me',\n",
    "        labelIds=[label_id]\n",
    "    ).execute()\n",
    "    if 'threads' not in thread_list_json: return \"\"\n",
    "    return set([d.get('id') for d in thread_list_json['threads']])\n",
    "\n",
    "# Note: We fetch all threads with label, then all messages in those threads, \n",
    "# then filter by date. This keeps unlabeled replies in labeled threads.\n",
    "# Using q='after:X before:Y' with labelIds would miss those unlabeled messages.\n",
    "# (A labled thread doesn't necessarily tag each message in the thread with the label.)\n",
    "def get_messages_for_label(label_key): # â†’ returns raw message dicts (all dates)\n",
    "    service = get_gmail_service()\n",
    "    label_ids=get_gmail_labels(service)\n",
    "    label_id=label_ids[label_key]\n",
    "    thread_ids=get_thread_ids(service,label_id)\n",
    "    msg_dicts=[]\n",
    "    for thread_id in thread_ids:\n",
    "        thread = service.users().threads().get(userId='me', id=thread_id).execute()\n",
    "        for msg in thread['messages']:\n",
    "            text_data, timestamp, threadId, sender = get_message_text(service, msg['id'])\n",
    "            if text_data:\n",
    "                msg_dicts.append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'threadId': threadId,\n",
    "                    'sender': sender,\n",
    "                    'content': extract_new_content(text_data)\n",
    "                })\n",
    "    return msg_dicts\n",
    "\n",
    "def filter_by_date(msgs, date):\n",
    "    start_ms, end_ms=start_stop_ms(date,1)\n",
    "    return [m for m in msgs if m['timestamp'] and (start_ms <= int(m['timestamp']) < end_ms)]\n",
    "\n",
    "def format_messages(msgs): # â†’ does the sorting and string building\n",
    "    sorted_msgs = sorted(msgs, key=lambda x: (x['threadId'], x['timestamp']))\n",
    "    sub_strs = [f\"From: {d['sender']}: \\n\\n{d['content']}\\n\\n=================\" \n",
    "                for d in sorted_msgs]\n",
    "    return '\\n\\n'.join(sub_strs)\n",
    "\n",
    "def get_daily_messages(label_key, date:str):\n",
    "    \"\"\"Get all messages for a given label and date.\"\"\"\n",
    "    msg_dicts = get_messages_for_label(label_key)\n",
    "    msgs = filter_by_date(msg_dicts, date)\n",
    "    return format_messages(msgs)\n",
    "\n",
    "def get_gmail_labels(service=None):\n",
    "    \"\"\"dict of label_id's by label name.lower()\"\"\"\n",
    "    if service is None: service = get_gmail_service()\n",
    "    results = service.users().labels().list(userId='me').execute()\n",
    "    labels = results.get('labels', [])\n",
    "    return {l['name'].lower(): l['id'] for l in labels if l['type']=='user'}\n",
    "\n",
    "def label_keys(): \n",
    "    return list(get_gmail_labels())\n",
    "'''\n",
    "Path('emails.py').write_text(emails_code)\n",
    "import emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c7709",
   "metadata": {
    "hide_input": true,
    "pinned": true,
    "time_run": "2025-12-02T06:03:50.540928+00:00"
   },
   "outputs": [],
   "source": [
    "logs_code=r'''\n",
    "from google_auth import get_gdocs_service\n",
    "from date_utils import str_to_date\n",
    "\n",
    "def get_paragraph_style(part):\n",
    "    return (part.get('paragraph', {})\n",
    "                .get('paragraphStyle', {})\n",
    "                .get('namedStyleType'))\n",
    "\n",
    "def is_heading_level(level:int,part): \n",
    "    return get_paragraph_style(part)==f'HEADING_{level}'\n",
    "\n",
    "def get_content(part):\n",
    "    els = part.get('paragraph',{}).get('elements','')\n",
    "    return ''.join([e.get('textRun',{}).get('content','') for e in els])\n",
    "\n",
    "def get_parts_list(service,doc_id):\n",
    "    doc = service.documents().get(documentId=doc_id).execute()\n",
    "    return doc['body']['content']\n",
    "\n",
    "def prepend_str(service, log_doc_id, pp_str, style='NORMAL_TEXT'):\n",
    "    requests = [{\n",
    "        'insertText': {\n",
    "            'location': {'index': 1},\n",
    "            'text': pp_str\n",
    "            }\n",
    "        },{\n",
    "            'updateParagraphStyle': {\n",
    "                'range': {'startIndex': 1, 'endIndex': len(pp_str)},\n",
    "                'paragraphStyle': {'namedStyleType': style},\n",
    "                'fields': 'namedStyleType'\n",
    "            }\n",
    "        }]\n",
    "    service.documents().batchUpdate(documentId=log_doc_id, body={'requests': requests}).execute()\n",
    "\n",
    "LOGS = {\n",
    "    \"assembly\": \"1Jb1LEU_VVorhIkslgkGOUgRADny23Z1Q310GJSo6ZWg\",\n",
    "    \"wcp\": \"1S4te4jvQGokB4cSbbInO9HuQUCEY6MHRontdIplXOBo\",\n",
    "    \"bench\": \"1ezsnTsRJQE8NkVId5sOqXowKopygoov9xzIl35oKczk\",\n",
    "    \"singulation\": \"1C6pDV0YWWwQra3sxPSRatdNV8DjPPdeBOvzxylLIUok\",\n",
    "    \"sterilization\": \"1HUfc25DiOzTIVqnFQ0FEqNqEaxs7eTosMxVKJCov9KQ\",\n",
    "    \"algorithm\": \"1nxCES9tzy2ZC0aFN-B08LO5lSs4YYVsgplED0v6GXxU\",\n",
    "    \"firmware\": \"1KF8LlS0UbJCdVbcqp_7BEwScJQc4EHwfI6fxNCU-SKE\",\n",
    "    \"quality\": \"1uXwbWHT3PjMFg6CJCIPs8ttQYTEdE4SYqm8wmH_AIvA\",\n",
    "    \"clinicals\": \"1EXJ4kd531ewCqZpD-aPqKdlMfVmRmQ9LIRsppUhSLuk\",\n",
    "    \"submission\": \"1Bbr8U9iDfYPrnoFKNeMgeMxYTN6uyeTTan5cvX_WmDI\"\n",
    "}\n",
    "\n",
    "def log_to_dict_by_date(parts_list):\n",
    "    log_dict = {}\n",
    "    key = None\n",
    "    year = None\n",
    "    for part in parts_list:\n",
    "        content = get_content(part).strip()\n",
    "        if is_heading_level(2,part):\n",
    "            year = int(content)\n",
    "        if is_heading_level(3,part) and content:\n",
    "            key = str_to_date(content, year)\n",
    "            log_dict[key] = ''\n",
    "        else: \n",
    "            if key is not None: \n",
    "                log_dict[key] += content\n",
    "    return log_dict\n",
    "\n",
    "def get_log_entries_in_range(log_dict,start_date,end_date):\n",
    "    return {str(k): v for k,v in log_dict.items() #str(date) for json\n",
    "            if start_date <= k <= end_date}\n",
    "\n",
    "def get_log_by_date(service,log_key, start_date, end_date):\n",
    "    log_id = LOGS[log_key]\n",
    "    parts_list = get_parts_list(service,log_id)\n",
    "    log_dict = log_to_dict_by_date(parts_list)\n",
    "    return get_log_entries_in_range(log_dict, start_date, end_date)\n",
    "\n",
    "def read_logs(start_date,end_date):\n",
    "    gdocs=get_gdocs_service()\n",
    "    return {k: get_log_by_date(gdocs,k,start_date,end_date)\n",
    "            for k in list(LOGS)}\n",
    "\n",
    "def log_keys():\n",
    "    return LOGS.keys()\n",
    "\n",
    "def write_log(key,digest,date):\n",
    "    docs = get_gdocs_service()\n",
    "    log_id=LOGS[key]\n",
    "    prepend_str(docs, log_id, digest + \"\\n\", 'NORMAL_TEXT')\n",
    "    prepend_str(docs, log_id, f\"{date}\\n\", 'HEADING_3')\n",
    "'''\n",
    "Path('logs.py').write_text(logs_code)\n",
    "import logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd3038",
   "metadata": {
    "hide_input": true,
    "pinned": true,
    "time_run": "2025-12-02T06:03:50.569219+00:00"
   },
   "outputs": [],
   "source": [
    "qdocs_code='''\n",
    "from google_auth import get_gsheets_service\n",
    "\n",
    "class GSheet:\n",
    "    \"A google sheet\"\n",
    "    def __init__(self,sheet_id:str,tab_range:str):\n",
    "        self.sheet_id=sheet_id # gsheet document id\n",
    "        self.tab_range=tab_range #gsheet tab & range\n",
    "\n",
    "    def get_rows(self):\n",
    "        gsheets=get_gsheets_service()\n",
    "        result = gsheets.spreadsheets().values().get(\n",
    "            spreadsheetId=self.sheet_id,\n",
    "            range=self.tab_range\n",
    "        ).execute()\n",
    "        return result.get('values',[])\n",
    "\n",
    "    def append(self,rows):\n",
    "        gsheets=get_gsheets_service()\n",
    "        gsheets.spreadsheets().values().append(\n",
    "            spreadsheetId=self.sheet_id,\n",
    "            range=self.tab_range,  \n",
    "            valueInputOption='RAW',  # or 'USER_ENTERED'\n",
    "            body={'values': rows}\n",
    "        ).execute()\n",
    "\n",
    "class QDoc(GSheet):\n",
    "    \"Quality Document (Risk Register, Decision Log, Design I/O Matrix)\"\n",
    "    def __init__(self, sheet_id, tab_range, entry_f):\n",
    "        super().__init__(sheet_id, tab_range)\n",
    "        self.entry_f = entry_f #doc entry defn function\n",
    "        self.rows = self.get_rows()\n",
    "        self._headers = self.rows[0] if self.rows else []\n",
    "\n",
    "    @staticmethod\n",
    "    def _sort_by_date_if_present(prop_dicts):\n",
    "        if not prop_dicts or 'Date' not in prop_dicts[0]: return prop_dicts\n",
    "        return sorted(prop_dicts, key=lambda d: d.get('Date', ''))\n",
    "\n",
    "    def _mk_row(self,prop):\n",
    "        return [prop.get(h,'') for h in self._headers]\n",
    "\n",
    "    def add_entries(self,props):\n",
    "        props=self._sort_by_date_if_present(props)\n",
    "        rows=[self._mk_row(prop) for prop in props]\n",
    "        self.append(rows)\n",
    "\n",
    "def qdocs(): \n",
    "    def risk_f(hazard:str, harms:str, causes:str)->dict:\n",
    "        \"Candidate Risk Register entry\"\n",
    "        return {'Hazard': hazard,\n",
    "            'Potential Harms': harms,\n",
    "            'Possible Causes': causes,\n",
    "            'Reviewed': 'FALSE'}\n",
    "\n",
    "    def decision_f(component:str,decision:str,rationale:str,date:str)->dict:\n",
    "        \"Candidate Decision Log entry\"\n",
    "        return {'Component': component,\n",
    "            'Decision': decision,\n",
    "            'Rationale': rationale,\n",
    "            'Date': date,\n",
    "            'Reviewed': 'FALSE'}\n",
    "\n",
    "    def design_f(design_input:str,source_rationale:str,design_output:str,verif_valid_method:str)->dict:\n",
    "        \"Candidate Design Input/Output Matrix entry\"\n",
    "        return {'Design Input': design_input,\n",
    "            'Source / Rationale': source_rationale,\n",
    "            'Design Output': design_output,\n",
    "            'Verification/Validation Method': verif_valid_method,\n",
    "            'Reviewed': 'FALSE'}\n",
    "\n",
    "    rreg = QDoc(\n",
    "        sheet_id='1j752K9N89Qv9WLkjje3OWkrzDbSMjLwYuxdoTfArcpQ',\n",
    "        tab_range='Risk Register!A:L',\n",
    "        entry_f=risk_f)\n",
    "    dlog = QDoc(\n",
    "        sheet_id='14oGD5hwGP_8OLfq_tqQVdEjicRzJ4cyQWPkXz92avv4',\n",
    "        tab_range='Decision Log!A:G',\n",
    "        entry_f=decision_f)   \n",
    "    dio = QDoc(\n",
    "        sheet_id='1xrV_39iaVoTpAXmbxbKfFBBnbdHN5yeyKJMDZ59MdqM',\n",
    "        tab_range='Design IO Matrix!A:G',\n",
    "        entry_f=design_f)\n",
    "    return [rreg,dlog,dio]\n",
    "\n",
    "'''\n",
    "Path('qdocs.py').write_text(qdocs_code)\n",
    "import qdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec83c17",
   "metadata": {
    "hide_input": true,
    "time_run": "2025-12-02T06:03:50.597954+00:00"
   },
   "outputs": [],
   "source": [
    "core_code=r'''\n",
    "from claudette import Chat, Client\n",
    "from date_utils import ytd\n",
    "from emails import label_keys, get_daily_messages\n",
    "from logs import log_keys, write_log, read_logs\n",
    "from qdocs import qdocs\n",
    "\n",
    "# update logs\n",
    "\n",
    "def get_email_cleaner():\n",
    "    instr = \"\"\"Please format this as a clean plain-text chronological transcript by:\n",
    "    1. Removing all email signature blocks\n",
    "    2. Removing any quoted/forwarded text\n",
    "    3. For each message: sender's name, colon, then the message body\n",
    "    4. After each message: \"---\" with no newlines or returns (no blank lines)\n",
    "    5. Keep it tight - no additional blank lines\n",
    "    6. Plain text only - no markdown formatting\"\"\"\n",
    "    return Chat(model='claude-sonnet-4-20250514', sp=instr)\n",
    "\n",
    "def clean(messages):\n",
    "    c = get_email_cleaner()\n",
    "    r = c(messages)\n",
    "    return r.content\n",
    "\n",
    "def update_logs(date=None):\n",
    "    \"\"\"update all logs based on emails from date (default yesterday)\"\"\"\n",
    "    if date is None: date=ytd()\n",
    "    for k in (set(label_keys()) & set(log_keys())): \n",
    "        messages = get_daily_messages(k, date)\n",
    "        if not messages: return\n",
    "        clean_digest = clean(messages)\n",
    "        if not clean_digest: return\n",
    "        write_log(k,clean_digest,date)\n",
    "\n",
    "## Update quality docs \n",
    "\n",
    "def _entries_prompt(logs):\n",
    "    p = f\"\"\"Review this set of engineering logs from a continuous glucose monitor development project\n",
    "            and identify candidate entries to the quality document represented in the tool. Focus on items \n",
    "            of significant, long-term importance; ignore short-term operational or execution issues. Keep \n",
    "            each field terse (10-20 words max).\n",
    "\n",
    "            Logs:\n",
    "            {logs} \n",
    "        \"\"\"\n",
    "    return p\n",
    "\n",
    "def _get_props(logs, qdoc):\n",
    "    \"\"\"propose entries for qdoc before seeing existing entries\"\"\"\n",
    "    c = Client('claude-sonnet-4-5')\n",
    "    return c.structured(_entries_prompt(logs),[qdoc.entry_f])\n",
    "\n",
    "def _filter_props(props, qdoc):\n",
    "    p = f\"\"\"\n",
    "    Determine which proposed new entries are NOT already documented the given existing document. \n",
    "    Return ONLY those proposals that represent genuinely NEW concepts not already captured in existing entries. \n",
    "    - Ignore wording differences - focus on whether the core concept is already documented.\n",
    "    - If a proposal is semantically similar to an existing entry, exclude it.\n",
    "    - If NONE of the proposals are new, make NO tool calls.\n",
    "\n",
    "    Proposals:\n",
    "    {props}\n",
    "\n",
    "    Existing entries:\n",
    "    {qdoc.rows}\n",
    "    \"\"\"\n",
    "    c = Client('claude-sonnet-4-5')\n",
    "    return c.structured(p,qdoc.entry_f)\n",
    "\n",
    "def update_qdocs(start_date,end_date):\n",
    "    entries=read_logs(start_date,end_date)\n",
    "    if not any(entries.values()): return\n",
    "    for qdoc in qdocs(): \n",
    "        props = _get_props(entries,qdoc)\n",
    "        news = _filter_props(props,qdoc)\n",
    "        if news: qdoc.add_entries(news)\n",
    "\n",
    "'''\n",
    "Path('core.py').write_text(core_code)\n",
    "import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69cf9f",
   "metadata": {
    "collapsed": true,
    "hide_input": true,
    "time_run": "2025-12-02T06:03:51.040590+00:00"
   },
   "outputs": [],
   "source": [
    "cloud_code=r'''\n",
    "# CLOUD DEPLOYMENT: When creating main.py, combine all modules but replace google_auth functions\n",
    "# with the keyless versions below. \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from google.auth import default, iam\n",
    "from google.auth.transport import requests as google_requests\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "SERVICE_ACCOUNT = 'docs-updater@qa-assistant-458920.iam.gserviceaccount.com'\n",
    "USER_EMAIL = 'dan@onehealthbiosensing.com'\n",
    "\n",
    "def get_google_service(service_name, version, scopes, user_email=USER_EMAIL):\n",
    "    \"\"\"Get Google API service using service account with delegation via IAM signJwt API.\"\"\"\n",
    "    signing_creds, _ = default()\n",
    "    signer = iam.Signer(\n",
    "        request=google_requests.Request(),\n",
    "        credentials=signing_creds,\n",
    "        service_account_email=SERVICE_ACCOUNT\n",
    "    )\n",
    "    delegated_creds = service_account.Credentials(\n",
    "        signer=signer,\n",
    "        service_account_email=SERVICE_ACCOUNT,\n",
    "        token_uri='https://oauth2.googleapis.com/token',\n",
    "        scopes=scopes,\n",
    "        subject=user_email\n",
    "    )\n",
    "    return build(service_name, version, credentials=delegated_creds)\n",
    "\n",
    "def get_gmail_service():\n",
    "    return get_google_service('gmail', 'v1', \n",
    "        ['https://www.googleapis.com/auth/gmail.readonly'])\n",
    "\n",
    "def get_gdocs_service():\n",
    "    return get_google_service('docs', 'v1', \n",
    "        ['https://www.googleapis.com/auth/documents'])\n",
    "\n",
    "def get_gsheets_service():\n",
    "    return get_google_service('sheets', 'v4', \n",
    "        ['https://www.googleapis.com/auth/spreadsheets'])\n",
    "\n",
    "# Cloud Function entry points\n",
    "\n",
    "def update_logs_daily(request):\n",
    "    \"\"\"Cloud Function entry point - updates logs from yesterday's emails.\"\"\"\n",
    "    from core import update_logs\n",
    "    update_logs()\n",
    "    return 'Logs updated successfully'\n",
    "\n",
    "def update_qdocs_weekly(request):\n",
    "    \"\"\"Cloud Function entry point - run Fridays at 4am for prior week (Fr-Th).\"\"\"\n",
    "    from core import update_qdocs\n",
    "    now = datetime.now(ZoneInfo('US/Central'))\n",
    "    end_date = (now - timedelta(hours=6)).date()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    update_qdocs(start_date, end_date)\n",
    "    return f\"Quality docs updated through {end_date.strftime('%Y/%m/%d')}\"\n",
    "'''\n",
    "Path('cloud.py').write_text(cloud_code)\n",
    "import cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386c00a",
   "metadata": {
    "hide_input": true,
    "time_run": "2025-12-02T06:03:51.372143+00:00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_md='''\n",
    "# Google Cloud setup:\n",
    "\n",
    "**One-Time Project Setup:**\n",
    "- [ ] Enable APIs: Cloud Run, Cloud Scheduler, Secret Manager, Gmail, Google Docs, Google Sheets, IAM Service Account Credentials\n",
    "- [ ] Enable billing and set budget alert\n",
    "\n",
    "**Service Account Configuration:**\n",
    "- [ ] Create service account: `docs-updater@qa-assistant-458920.iam.gserviceaccount.com`\n",
    "- [ ] Grant **Service Account Token Creator** role to itself (for keyless JWT signing)\n",
    "- [ ] Configure domain-wide delegation in Google Workspace Admin Console with scopes: `gmail.readonly`, `documents`, `spreadsheets`\n",
    "\n",
    "**Secret Management:**\n",
    "- [ ] Create secret `anthropic-api-key` in Secret Manager\n",
    "- [ ] Grant `docs-updater` service account **Secret Manager Secret Accessor** role\n",
    "\n",
    "**Per-Function Deployment:**\n",
    "- [ ] Deploy function with runtime service account set to `docs-updater`\n",
    "- [ ] Reference `anthropic-api-key` as environment variable\n",
    "- [ ] Grant `docs-updater` **Cloud Run Invoker** role on the deployed function\n",
    "\n",
    "**Scheduler Setup:**\n",
    "- [ ] Create job with OIDC auth using `docs-updater` service account\n",
    "- [ ] Set schedule and timezone appropriately\n",
    "'''\n",
    "Path('gcloud.md').write_text(cloud_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6c5b9",
   "metadata": {
    "time_run": "2025-12-02T06:03:51.409828+00:00"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from core import update_qdocs\n",
    "update_qdocs(date(2025,10,3),date(2025,10,9))"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
